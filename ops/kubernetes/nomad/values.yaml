# Default values for nomad.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
roll: false
nameOverride: ''
fullnameOverride: ''

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ''

nomad:
  enabled: true
  image:
    repository: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-distro
    tag: latest
    pullPolicy: Always

  imagePullSecrets: []
  config:
    version:
      label: latest
      isTest: false
      isBeta: false
      usesBetaData: false
      officialUrl: https://nomad-lab.eu/prod/v1/gui

    meta:
      service: app
      homepage: https://nomad-lab.eu
      source_url: https://gitlab.mpcdf.mpg.de/nomad-lab/nomad-distro
      maintainer_email: markus.scheidgen@physik.hu-berlin.de

    api:
      ## Secret used as cryptographic seed
      secret: defaultApiSecret
      ## Limit of unpublished uploads per user, except admin user
      uploadLimit: 10

    app:
      console_loglevel: INFO
      logstash_loglevel: INFO

    worker:
      console_loglevel: ERROR
      logstash_loglevel: INFO
      ## There are two routing modes "queue" and "worker". The "queue" routing will use a general
      # task queue and spread calc processing task over worker instances. The "worker" routing
      # will send all tasks related to an upload to the same worker
      routing: queue
      timeout: 7200
      acks_late: false

    mail:
      enabled: false
      host: localhost
      port: 25
      from: support@nomad-lab.eu

    client:
      username: admin

    springerDbPath: /nomad/fairdi/db/data/springer.msg

    reprocess:
      rematchPublished: true
      reprocessExistingEntries: true
      useOriginalParser: false
      addMatchedEntriesToPublished: false
      deleteUnmatchedPublishedEntries: false
      indexIndividualEntries: false

    process:
      reuseParser: true
      indexMaterials: true
      rfc3161_skip_published: false

    datacite:
      enabled: false
      prefix: '10.17172'

    ## A common name/prefix for all dbs and indices.
    dbname: fairdi_nomad_latest

    mongo:
      host: ''
      port: 27017

    elastic:
      host: ''
      port: 9200
      timeout: 60
      bulkTimeout: 600
      bulkSize: 1000
      entriesPerMaterialCap: 1000

    logstash:
      enabled: true
      port: 5000
      host: ''

    keycloak:
      serverExternalUrl: https://nomad-lab.eu/fairdi/keycloak/auth/
      serverUrl: https://nomad-lab.eu/keycloak/auth/
      realmName: fairdi_nomad_test
      username: admin
      clientId: nomad_public
      guiClientId: nomad_public
      admin_user_id: 00000000-0000-0000-0000-000000000000

    ## Everything concerning the data that is used by the service
    volumes:
      prefixSize: 1
      public: /nomad/fairdi/latest/fs/public
      staging: /nomad/fairdi/latest/fs/staging
      north_home: /nomad/fairdi/latest/fs/north/users
      tmp: /nomad/fairdi/latest/fs/tmp
      nomad: /nomad

    services:
      aitoolkit:
        ## enable aitoolkit references
        enabled: false

    north:
      enabled: false
      hubServiceApiToken: secret-token

    gui:
      ## This variable is used in the GUI to show or hide additional information
      debug: false
      ## automatically gz based on header
      gzip: true

    proxy:
      # Set a nodePort to create a NodePort service instead of ClusterIP. Also set a nodeIP for the externalIP.
      timeout: 120
      editTimeout: 1800
      external:
        host: nomad-lab.eu
        port: 80
        path: /fairdi/nomad/latest
        https: true


  ingress:
    enabled: false
    limitConnections: 32
    limitConnectionsApi: 8
    hosts:
    - nomad-lab.eu
    className: ''
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: 'false'
      nginx.ingress.kubernetes.io/proxy-body-size: 32g
    tls: []

  # Additional volumes on the output Deployment definition.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  # APPLICATION SPECIFIC parameters
  service:
    type: ClusterIP
    port: 80

  # TODO: Do we really need this? different app could bave their own ingress config. Eventually prozy is just another nginx ssrver
  ## Everything concerning the nginx that serves the gui, proxies the api
  #  It is run via NodePort service
  proxy:
    path: /fairdi/nomad/latest
    timeout: 60
    editTimeout: 60
    connectionTimeout: 10

    replicaCount: 1
    # Set a nodePort to create a NodePort service instead of ClusterIP. Also set a nodeIP for the externalIP.

    image:
      repository: nginx
      # Overrides the image tag whose default is the chart appVersion.
      tag: 1.13.9-alpine
      pullPolicy: IfNotPresent

    command: [nginx]
    args: [-g, daemon off;]

    imagePullSecrets: []

    service:
      type: ClusterIP
      port: 80

    podSecurityContext: {}
      # fsGroup: 2000

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # Additional volumes on the output Deployment definition.
    volumes: []
    # - name: foo
    #   secret:
    #     secretName: mysecret
    #     optional: false

    # Additional volumeMounts on the output Deployment definition.
    volumeMounts: []
    # - name: foo
    #   mountPath: "/etc/foo"
    #   readOnly: true

    nodeSelector: {}
    tolerations: []
    affinity: {}

    podAnnotations: {}
    podLabels: {}

  ## Everything concerning the nomad app
  app:
    replicaCount: 1

    # options: {}
    service:
      type: ClusterIP
      port: 8000

    podSecurityContext: {}
      # fsGroup: 2000

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # Additional volumes on the output Deployment definition.
    volumes: []
    # - name: foo
    #   secret:
    #     secretName: mysecret
    #     optional: false

    # Additional volumeMounts on the output Deployment definition.
    volumeMounts: []
    # - name: foo
    #   mountPath: "/etc/foo"
    #   readOnly: true

    nodeSelector: {}
    tolerations: []
    affinity: {}

    podAnnotations: {}
    podLabels: {}

  ## Everything concerning the nomad worker
  worker:
    replicaCount: 1

    maxTasksPerChild: 128
    # storage: "disk"

    podSecurityContext: {}
      # fsGroup: 2000

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    # request and limit in GB, good prod sizes are 64, 420
    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # Additional volumes on the output Deployment definition.
    volumes: []
    # - name: foo
    #   secret:
    #     secretName: mysecret
    #     optional: false

    # Additional volumeMounts on the output Deployment definition.
    volumeMounts: []
    # - name: foo
    #   mountPath: "/etc/foo"
    #   readOnly: true

    nodeSelector: {}
    tolerations: []
    affinity: {}

    podAnnotations: {}
    podLabels: {}

  ## Everything concerning the nomad adminconsole
  adminconsole:
    enabled: false

    replicaCount: 1

    podSecurityContext: {}
      # fsGroup: 2000

    securityContext: {}
      # capabilities:
      #   drop:
      #   - ALL
      # readOnlyRootFilesystem: true
      # runAsNonRoot: true
      # runAsUser: 1000

    # request and limit in GB, good prod sizes are 64, 420
    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # Additional volumes on the output Deployment definition.
    volumes: []
    # - name: foo
    #   secret:
    #     secretName: mysecret
    #     optional: false

    # Additional volumeMounts on the output Deployment definition.
    volumeMounts: []
    # - name: foo
    #   mountPath: "/etc/foo"
    #   readOnly: true

    nodeSelector: {}
    tolerations: []
    affinity: {}

    podAnnotations: {}
    podLabels: {}

  ## Everthing concerning the nomad gui
  gui:
    ## This variable is used in the GUI to show or hide additional information
    debug: false
    ## automatically gz based on header
    gzip: true
    ## configuration for the interface, menus, options, etc.
    config: {}

rabbitmq:
  persistence:
    enabled: false
  image.pullSecrets: nil
  auth:
    username: rabbitmq
    password: rabbitmq
    erlangCookie: SWQOKODSQALRPCLNMEQG

jupyterhub:
  debug:
    enabled: false
  # fullnameOverride: null
  # nameOverride: "north"
  proxy:
    service:
      type: ClusterIP
  singleuser:
    image:
      pullPolicy: Always
    storage:
      type: none
  hub:
    extraEnv:
      NOMAD_NORTH_HUB_SERVICE_API_TOKEN:
        valueFrom:
          secretKeyRef:
            name: nomad-hub-service-api-token
            key: token
    allowNamedServers: true
    shutdownOnLogout: true
    config:
      JupyterHub:
        authenticator_class: generic-oauth
      Authenticator:
        auto_login: true
        enable_auth_state: true
      GenericOAuthenticator:
        client_id: nomad_public
        oauth_callback_url:
        authorize_url: 
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/auth
        token_url: 
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/token
        userdata_url: 
          https://nomad-lab.eu/fairdi/keycloak/auth/realms/fairdi_nomad_test/protocol/openid-connect/userinfo
        login_service: keycloak
        username_key: preferred_username
        userdata_params:
          state: state
    extraConfig:
      01-prespawn-hook.py: |
        import os
        import requests
        import asyncio

        hub_service_api_token = os.getenv('NOMAD_NORTH_HUB_SERVICE_API_TOKEN')

        # configure nomad service
        c.JupyterHub.services.append(
            {
                "name": "nomad-service",
                "admin": True,
                "api_token": hub_service_api_token,
            }
        )

        async def pre_spawn_hook(spawner):
            await spawner.load_user_options()
            username = spawner.user.name

            spawner.log.info(f"username: {username}")
            spawner.log.debug(f'Configuring spawner for named server {spawner.name}')

            if spawner.handler.current_user.name != 'nomad-service':
                # Do nothing, will only launch the default image with no volumes.
                return

            user_home = spawner.user_options.get('user_home')
            spawner.log.info(f"user_home: {user_home}")
            if user_home:
                spawner.volumes.append({
                    'name': 'user-home',
                    'hostPath': {'path': user_home['host_path']}
                })
                spawner.volume_mounts.append({
                    'name': 'user-home',
                    'mountPath': user_home['mount_path'],
                    'readOnly': False
                })

            uploads = spawner.user_options.get('uploads', [])
            spawner.log.info(f"uploads: {uploads}")
            for (i, upload) in enumerate(uploads):
                spawner.volumes.append({
                    'name': f"uploads-{i}",
                    'hostPath': {'path': upload['host_path']}
                })
                spawner.volume_mounts.append({
                    'name': f"uploads-{i}",
                    'mountPath': upload['mount_path'],
                    'readOnly': False
                })

            environment = spawner.user_options.get('environment', {})
            spawner.environment.update(environment)

            tool = spawner.user_options.get('tool')
            if tool:
                spawner.image = tool.get('image')
                spawner.cmd = tool.get('cmd')

                # Workaround for webtop based images (no connection to jupyterhub itself)
                if tool.get('privileged'):
                    spawner.privileged = True
                    spawner.allow_privilege_escalation = True
                    spawner.uid = 0

        c.Spawner.pre_spawn_hook = pre_spawn_hook
        c.OAuthenticator.allow_all = True

  cull:
    enabled: true
    timeout: 86400  # 24 hours
    every: 600
    removeNamedServers: true

  prePuller:
    hook:
      enabled: true
      image:
        pullPolicy: Always
    continuous:
      enabled: false
    extraImages:
      jupyter:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/north/jupyter
        tag: latest
      pyiron:
        name: pyiron/pyiron
        tag: latest
      nionswift:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/nionswift-webtop
        tag: latest
      nexustools:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/nexus-webtop
        tag: latest
      ellips:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/north/ellips/jupyter
        tag: latest
      mpes:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/north/mpes/webtop
        tag: latest
      xps:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/north/xps/jupyter
        tag: master
      sts:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/sts-jupyter
        tag: latest
      webtop:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/webtop
        tag: latest
      apmtools:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/apmtools-webtop
        tag: latest
      fiji:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/fiji-webtop
        tag: latest
      frwr:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/frwr-webtop
        tag: latest
      abtem:
        name: gitlab-registry.mpcdf.mpg.de/nomad-lab/nomad-remote-tools-hub/abtem-webtop
        tag: latest
  scheduling:
    userScheduler:
      enabled: false
    podPriority:
      enabled: false
    userPlaceholder:
      enabled: false
      replicas: 0

mongodb:
  enabled: true

elasticsearch:
  enabled: true
